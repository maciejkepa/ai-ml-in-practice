{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4136ee6d-5560-44c0-8005-c6db9ad62be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee471d1-8545-400c-9ddc-d29a62f263c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../environment/prepare_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dba3fdb-82a4-4d4a-9985-832d1d073947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare utility functions\n",
    "\n",
    "\n",
    "The following utility functions are used during the Bronze â†’ Silver write operations:\n",
    "* **add_technical_columns**  \n",
    "  Adds technical metadata columns such as `_loaded_at` used for ingestion tracking.\n",
    "* **get_max_loaded_at**  \n",
    "  Retrieves the maximum `_loaded_at` timestamp from the provided DataFrame.\n",
    "* **build_merge_condition**  \n",
    "  Constructs a SQL merge condition based on key columns joining source and target tables.\n",
    "* **update_processed_flag**  \n",
    "  Updates the `_is_processed` flag for records that were loaded on or before the specified timestamp.\n",
    "* **merge_into_table**  \n",
    "  Executes a Delta MERGE operation to upsert records and optionally delete unmatched target rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490b0445-e67c-47ae-90fd-2f56a5456024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from delta import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "spark.sql(\"USE CATALOG ai_ml_in_practice\")\n",
    "\n",
    "\n",
    "def add_technical_columns(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"_loaded_at\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "def get_max_loaded_at(df: DataFrame) -> datetime:\n",
    "\n",
    "    max_loaded_at_row = df.agg(\n",
    "        F.max(\"_loaded_at\").alias(\"MAX_LOADED_AT\")\n",
    "    ).collect()\n",
    "    max_loaded_at = max_loaded_at_row[0][\"MAX_LOADED_AT\"]\n",
    "\n",
    "    return max_loaded_at\n",
    "\n",
    "def build_merge_condition(\n",
    "    merge_keys: list[str], target_alias: str = \"target\", source_alias: str = \"source\") -> str:\n",
    "    return \" AND \".join(\n",
    "        [f\"{target_alias}.{key} = {source_alias}.{key}\" for key in merge_keys]\n",
    "    )\n",
    "\n",
    "def update_processed_flag(table: str, max_loaded_at: datetime) -> None:\n",
    "    delta_table = DeltaTable.forName(spark, table)\n",
    "\n",
    "    delta_table.update(\n",
    "        condition=(F.col(\"_is_processed\") == F.lit(False))\n",
    "        & (F.col(\"_loaded_at\") <= max_loaded_at),\n",
    "        set={\"_is_processed\": F.lit(True)},\n",
    "    )\n",
    "\n",
    "def merge_into_table(\n",
    "    df: DataFrame,\n",
    "    table: str,\n",
    "    merge_keys: list[str],\n",
    "    delete_unmatched_table_rows: bool = False,\n",
    ") -> None:\n",
    "    if not spark.catalog.tableExists(table):\n",
    "        logging.warning(\n",
    "            f\"Table {table} doesn't exist. Appending the DataFrame to the empty table\"\n",
    "        )\n",
    "        df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"false\").saveAsTable(\n",
    "            table\n",
    "        )\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Executing merge into Delta table: {table}\")\n",
    "    delta_table = DeltaTable.forName(spark, table)\n",
    "\n",
    "    merge_condition = build_merge_condition(\n",
    "        merge_keys=merge_keys, target_alias=\"table\", source_alias=\"df\"\n",
    "    )\n",
    "    logging.debug(f\"Using merge condition: {merge_condition}\")\n",
    "\n",
    "    update_columns = {\n",
    "        col: f\"df.{col}\" for col in df.columns if col not in [\"_loaded_at\"]\n",
    "    }\n",
    "\n",
    "    merge_builder = (\n",
    "        delta_table.alias(\"table\")\n",
    "        .merge(source=df.alias(\"df\"), condition=merge_condition)\n",
    "        .whenMatchedUpdate(set=update_columns)\n",
    "        .whenNotMatchedInsertAll()\n",
    "    )\n",
    "\n",
    "    if delete_unmatched_table_rows:\n",
    "        merge_builder = merge_builder.whenNotMatchedBySourceDelete()\n",
    "\n",
    "    merge_builder.execute()\n",
    "    logging.info(\"Merge operation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328030e5-4432-4812-963f-8f65f97354c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load bronze data to silver table.\n",
    "\n",
    "During data loading from bronze to silver layer, we will introduce several operations:\n",
    "* column names standardization\n",
    "* column types casting:\n",
    "  * `senior_citizen`, `partner`, `dependants`, `phone_service`, `internet_service`, `paperless_billing`, `churn` to `Boolean`\n",
    "  * `tenure` to `Integer`\n",
    "* handling missing data\n",
    "* outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c7e9a9-1235-4c8a-8d29-a3c16ae640fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "    customer_id as customer_id,\n",
    "    gender as gender,\n",
    "    seniorCitizen as senior_citizen,\n",
    "    partner as partner,\n",
    "    Dependents as dependents,\n",
    "    tenure as tenure,\n",
    "    Phone_Service as phone_service,\n",
    "    MultipleLines as multiple_lines,\n",
    "    InternetService as internet_service,\n",
    "    Online_Security as online_security,\n",
    "    Online_Backup as online_backup,\n",
    "    DeviceProtection as device_protection,\n",
    "    TechSupport as tech_support,\n",
    "    StreamingTV as streaming_tv,\n",
    "    StreamingMovies as streaming_movies,\n",
    "    Contract as contract,\n",
    "    paperlessbilling as paperless_billing,\n",
    "    paymentmethod as payment_method,\n",
    "    MonthlyCharges as monthly_charges,\n",
    "    TotalCharges as total_charges,\n",
    "    Churn as churn,\n",
    "    _loaded_at,\n",
    "    _is_processed,\n",
    "    _row_id\n",
    "    FROM ai_ml_in_practice.telco_customer_churn_bronze.telco_bronze\n",
    "    WHERE _is_processed = False\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6cebd31-c6ea-49eb-83dc-40cc1e3cc84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, ShortType, IntegerType\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Convert string columns to binary format\n",
    "binary_columns = [\"senior_citizen\", \"partner\", \"dependents\", \"phone_service\", \"internet_service\", \"paperless_billing\", \"churn\"]\n",
    "for column in binary_columns:\n",
    "    bronze_df = bronze_df.withColumn(column, col(column).cast(BooleanType()))\n",
    "\n",
    "# Convert tenure to integer\n",
    "bronze_df = bronze_df.withColumn(\"tenure\", col(\"tenure\").cast(IntegerType()))\n",
    "\n",
    "bronze_df.printSchema()\n",
    "display(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d560509-07d6-4576-b5a3-20783d6592fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Outliers\n",
    "\n",
    "Outliers are data points that fall far outside the typical range of values in a dataset. Common methods for handling outliers include removing them, filtering, transforming the data, or replacing outliers with more representative values.\n",
    "\n",
    "Identified outliers:\n",
    "* negative values in `total_charges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de00db42-5cae-49f4-8087-5d3f1fe5ad59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Use .filter method and SQL col() function\n",
    "bronze_df = bronze_df.filter((col(\"total_charges\") > 0) | (col(\"TotalCharges\").isNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "470c8d27-3316-44a2-86cf-96bd566d629b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "To handle missing values we need to identify places with high percentages of missing data. We can later either:\n",
    "* drop whole columns\n",
    "* drop certain rows\n",
    "* impute columns with some default value\n",
    "* impute numeric columns using statistical methods\n",
    "* treat null as categorical feature\n",
    "\n",
    "Identified problems:\n",
    "* `internet_service` will be imputed based on `online_security` column\n",
    "* rows with most missing columns will be removed\n",
    "* remove each row where `total_charge` or `tenure` is missing\n",
    "* impute `Boolean` columns with default `False` and `String` columns with `N/A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c24f91-c8d2-406b-a59e-10b6dfc10fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def count_nulls(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing the number of null values for each column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input Spark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Two-column DataFrame with column name and null count.\n",
    "    \"\"\"\n",
    "    null_counts = [\n",
    "        F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "        for c in df.columns\n",
    "    ]\n",
    "\n",
    "    return df.agg(*null_counts)\n",
    "\n",
    "display(count_nulls(bronze_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38204868-15ed-4a9c-a069-ab978aeb096e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imputation on internet_service column\n",
    "bronze_df = bronze_df.withColumn(\n",
    "    \"internet_service\",\n",
    "    F.when(F.col(\"online_security\") == \"No internet service\", F.lit(False))\n",
    "     .otherwise(F.lit(True))\n",
    ")\n",
    "\n",
    "display(count_nulls(bronze_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243282ad-da87-44be-8b09-8d9445f3da86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rows with missing data removal\n",
    "n_cols = len(bronze_df.columns)\n",
    "bronze_df = bronze_df.na.drop(how='any', thresh=round(n_cols*.30))\n",
    "\n",
    "display(count_nulls(bronze_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c590e03e-0fdc-495c-b27c-02532927b235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# String columns imputation\n",
    "string_cols = [c.name for c in bronze_df.schema.fields if c.dataType == StringType()]\n",
    "bronze_df = bronze_df.na.fill(value='N/A', subset=string_cols)\n",
    "\n",
    "display(count_nulls(bronze_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27682ce-d2cd-4cfe-8c63-503fa8f9a234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Boolean columns imputation\n",
    "bool_cols = [c.name for c in bronze_df.schema.fields if (c.dataType == BooleanType())]\n",
    "bronze_df = bronze_df.na.fill(value=False, subset=bool_cols)\n",
    "\n",
    "display(count_nulls(bronze_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24dcd58f-3eb1-475b-840b-363435b2c1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing tenure or total_charges\n",
    "bronze_df = bronze_df.na.drop(subset=[\"tenure\", \"total_charges\"])\n",
    "\n",
    "display(count_nulls(bronze_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc77a37-b2ac-4ca8-8dc1-5f513cd63bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Great Expectations\n",
    "\n",
    "Great Expectations validation created in `2.4_telco_great_expectations` will be used to validate the DataFrame before writing it to silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ebd007-4817-408b-81ec-6daf679a4830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import great_expectations as gx\n",
    "\n",
    "# Get GE context and validation definition\n",
    "context_root_dir = \"telco_ge\"\n",
    "context = gx.get_context(context_root_dir=context_root_dir)\n",
    "validation_definition = context.validation_definitions.get(\"telco_silver_validation\")\n",
    "\n",
    "batch_parameters = {\"dataframe\": bronze_df}\n",
    "\n",
    "validation_results = validation_definition.run(batch_parameters=batch_parameters)\n",
    "print(validation_results)\n",
    "\n",
    "if not validation_results.get(\"success\", False):\n",
    "    print(\"Validation failed! Stopping notebook execution.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4a0eff-0ca0-40ae-97fa-40d480b68b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get last bronze load date\n",
    "max_loaded_at = get_max_loaded_at(bronze_df)\n",
    "\n",
    "# Create silver dataframe\n",
    "silver_df = add_technical_columns(bronze_df).drop(\"_loaded_at\")\n",
    "\n",
    "# Merge into silver table\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS telco_customer_churn_silver\")\n",
    "merge_into_table(silver_df, \"ai_ml_in_practice.telco_customer_churn_silver.telco_silver\", [\"_row_id\"])\n",
    "\n",
    "# Update bronze table\n",
    "update_processed_flag(\"ai_ml_in_practice.telco_customer_churn_bronze.telco_bronze\", max_loaded_at)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.3_telco_bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
