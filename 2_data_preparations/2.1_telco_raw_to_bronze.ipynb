{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54274a25-f511-497f-86b4-6a70ec578891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818b5560-9e5b-478f-9de7-95128081e1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../environment/prepare_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a63877-febc-4a10-be21-b92c72a3a25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../environment/prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1f0e72-bcf6-49b4-967a-7d5bc8b23716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load raw data to bronze table.\n",
    "\n",
    "Bronze table preserves the schema as-is from the original while only adding some ingestion metadata like loading time, source file details and row ID.\n",
    "\n",
    "We use **Autoloader** (`cloudFiles`) for efficient, incremental ingestion of new files without scanning the entire directory.\n",
    "\n",
    "To learn more about Autoloader please check [Databricks Autoloader documentation](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/auto-loader/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cf8d7c-8b8f-4026-b756-faa0ba943e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "spark.sql(\"USE CATALOG ai_ml_in_practice\")\n",
    "\n",
    "# Prepare technical columns function\n",
    "def add_technical_columns(df: DataFrame) -> DataFrame:\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"_loaded_at\", F.current_timestamp())\n",
    "        .withColumn(\"_file_modified_at\", F.col(\"_metadata.file_modification_time\"))\n",
    "        .withColumn(\"_source_file\", F.col(\"_metadata.file_name\"))\n",
    "        .withColumn(\"_row_id\", F.expr(\"uuid()\"))\n",
    "        .withColumn(\"_is_processed\", F.lit(False))\n",
    "    )\n",
    "\n",
    "# Create volume to store Autoloader checkpoint files\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS telco_customer_churn_bronze.autoloader_files\")\n",
    "checkpoint_file_location = f\"/Volumes/ai_ml_in_practice/telco_customer_churn_bronze/autoloader_files\"\n",
    "\n",
    "# Run autoloader\n",
    "try:\n",
    "    (\n",
    "        spark.readStream                                                           # Init streaming source \n",
    "        .format(\"cloudFiles\")                                                      # Specify cloudFiles format to use Autoloader\n",
    "        .option(\"cloudFiles.format\", \"csv\")                                        # File type (csv)\n",
    "        .option(\"inferSchema\", \"true\")                                             # Auto schema detection\n",
    "        .option(\"delimiter\", \",\")                                                  # Set delimiter to \",\"\n",
    "        .option(\"escape\", '\"')                                                     # Set escape character to '\"'\n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint_file_location)             # Schema files path\n",
    "        .option(\"cloudFiles.maxFilesPerTrigger\", 1)                                # Max number of files processed per trigger\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")                             # Try to \"guess\" column types\n",
    "        .load(f\"/Volumes/ai_ml_in_practice/telco_customer_churn_raw/telco_data\")   # Source files path\n",
    "        .transform(add_technical_columns)                                          # Function to add technical columns\n",
    "        .writeStream.format(\"delta\")                                               # Output format (delta)\n",
    "        .option(\"checkpointLocation\", checkpoint_file_location)                    # Checkpoint files path\n",
    "        .trigger(availableNow=True)                                                # Process all available files now\n",
    "        .toTable(\"telco_customer_churn_bronze.telco_bronze\")                       # Output table path\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during Autoloader ingestion:\\n{e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8098765900390542,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.1_telco_raw_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
