{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4136ee6d-5560-44c0-8005-c6db9ad62be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee471d1-8545-400c-9ddc-d29a62f263c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../environment/prepare_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dba3fdb-82a4-4d4a-9985-832d1d073947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Great Expectations\n",
    "\n",
    "This notebook ensures the Silver layer data meets quality standards before any downstream processing by creating Great Expectations suite for validation.\n",
    "\n",
    "In details this notebook:\n",
    "* Loads the Great Expectations context for the Silver layer.\n",
    "* Creates an expectation suite for telco_silver data.\n",
    "* Adds expectations for required columns, boolean types, and non-negative numeric columns.\n",
    "* Sets up a Spark dataframe as the data source.\n",
    "* Defines a batch and validation run against the suite.\n",
    "\n",
    "**The requirements to register a table in Feature Store are that you need to remove the target column and  define a primary key for the table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490b0445-e67c-47ae-90fd-2f56a5456024",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765970425635}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the base silver table\n",
    "silver_df = spark.table(\"ai_ml_in_practice.telco_customer_churn_silver.telco_silver\")\n",
    "\n",
    "silver_df = silver_df.drop(\"churn\", \"_is_processed\", \"_row_id\")\n",
    "\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "284cb5e1-03d7-4d70-b25d-6fa516797c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Feature Store table for customer-level features\n",
    "\n",
    "This cell registers the prepared feature DataFrame as a **Feature Store table**.\n",
    "\n",
    "What happens here:\n",
    "- `FeatureEngineeringClient` is initialized to interact with Databricks Feature Store.\n",
    "- A new feature table is created from the current `silver_df`.\n",
    "- `customer_id` is defined as the primary key, making features joinable and reusable.\n",
    "- Metadata (description and tags) is attached for lineage and discoverability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c7e9a9-1235-4c8a-8d29-a3c16ae640fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "fe.create_table(\n",
    "    name=\"ai_ml_in_practice.telco_customer_churn_silver.telco_customer_features\",\n",
    "    primary_keys=[\"customer_id\"],\n",
    "    df=silver_df,\n",
    "    description=\"Telco customer features\",\n",
    "    tags={\"source\": \"silver\", \"format\": \"delta\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98149be3-f244-407e-8835-f8425d5c2cdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build a training dataset using Feature Store lookups\n",
    "\n",
    "Here we assemble the training dataset by joining labels with reusable features stored in the Feature Store.\n",
    "\n",
    "What happens step by step:\n",
    "- `FeatureLookup` defines which feature table to join and how (`customer_id`).\n",
    "- Only selected feature columns are pulled into the training set.\n",
    "- The base DataFrame provides the label (`churn`) and join key.\n",
    "- `create_training_set` materializes a point-in-timeâ€“consistent dataset.\n",
    "- `load_df()` returns a Spark DataFrame ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56dddb3c-c4d4-499d-a992-d6cabd2ef335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "feature_lookups = [\n",
    "  FeatureLookup(\n",
    "    table_name=\"ai_ml_in_practice.telco_customer_churn_silver.telco_customer_features\",\n",
    "    feature_names=[\"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\"],\n",
    "    lookup_key=\"customer_id\"\n",
    "  )\n",
    "]\n",
    "\n",
    "feature_df = spark.table(\"ai_ml_in_practice.telco_customer_churn_silver.telco_silver\").select(\"customer_id\", \"churn\")\n",
    "display(feature_df)\n",
    "\n",
    "training_set = fe.create_training_set(\n",
    "  df=feature_df,\n",
    "  feature_lookups=feature_lookups,\n",
    "  exclude_columns=[\"customer_id\"],\n",
    "  label=\"churn\",\n",
    ")\n",
    "\n",
    "training_df = training_set.load_df()\n",
    "display(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf97219-2674-482a-9a16-868afcebe729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transforming training dataset\n",
    "\n",
    "The same transformations done before on the base dataset can be done on assembled training dataset.\n",
    "\n",
    "As an example, we will perform embeddings encoding of internet services columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e52ca84-06d9-4f96-9ce1-eebcd2b668c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql.functions import split, concat_ws, col\n",
    "\n",
    "# Define categorical columns\n",
    "internet_categorical_columns = [\"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\"]\n",
    "training_df = training_df.withColumn(\"categorical_sequence\", concat_ws(\";\", *internet_categorical_columns))\n",
    "training_df = training_df.withColumn(\"categorical_tokens\", split(col(\"categorical_sequence\"), \";\"))\n",
    "\n",
    "model = mlflow.spark.load_model(\n",
    "    \"models:/ai_ml_in_practice.telco_customer_churn_silver.telco_word2vec_internet_services/1\",\n",
    "    dfs_tmpdir=\"/Volumes/ai_ml_in_practice/telco_customer_churn_silver/mlflow_tmp\"\n",
    ")\n",
    "\n",
    "# Generate embeddings for categorical columns\n",
    "training_df = model.transform(training_df)\n",
    "training_df.drop(\"categorical_sequence\", \"categorical_tokens\")\n",
    "\n",
    "display(training_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.2_telco_feature_store",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
