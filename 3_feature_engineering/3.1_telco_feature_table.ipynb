{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4136ee6d-5560-44c0-8005-c6db9ad62be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee471d1-8545-400c-9ddc-d29a62f263c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../environment/prepare_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328030e5-4432-4812-963f-8f65f97354c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform feature engineering steps to transform the data from silver layer\n",
    "\n",
    "In this section, we will perform several feature engineering steps to transform the dataset and save it as a feature table:\n",
    "* load silver Telco dataset\n",
    "* cast the integer and boolean columns to `Double` type\n",
    "* perform one-hot encoding on `gender` column\n",
    "* perform feature hashing on columns related to `phone service`\n",
    "* perform embedding encoding on columns related to `internet services` and `payment_method`\n",
    "* perform ordinal encoding on `contract` column\n",
    "* perform scaling on `tenure` and `charge` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c7e9a9-1235-4c8a-8d29-a3c16ae640fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.table(\"ai_ml_in_practice.telco_customer_churn_silver.telco_silver\").drop(\"customer_id\", \"_row_id\", \"_is_processed\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19fdf70-c962-4682-a943-d8c863a7d9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Type casing\n",
    "\n",
    "We scan the schema and find all columns that are integers or booleans.\n",
    "Then we explicitly cast them to `Double`.\n",
    "\n",
    "**Note: the final vector assebly will automatically convert these columns to `Double` without manual conversion. This step is done only for code / data sanity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56dddb3c-c4d4-499d-a992-d6cabd2ef335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Get a list of integer & boolean columns\n",
    "integer_cols = [column.name for column in silver_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "\n",
    "# Loop through integer columns to cast each one to double\n",
    "for column in integer_cols:\n",
    "    silver_df = silver_df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006c2239-5ac6-46ec-ae9f-369a83332c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### One-hot encoding\n",
    "\n",
    "To one-hot encode categorical features in Spark ML, we follow a simple but explicit three-step workflow built around Spark MLlib transformers.\n",
    "\n",
    "First, categorical values must be converted into numbers.\n",
    "This is handled by `StringIndexer`, which:\n",
    "- scans a string column,\n",
    "- assigns a unique numeric index to each distinct category,\n",
    "- defines how unseen values should be treated during transformation.\n",
    "\n",
    "Next, the indexed values are expanded using `OneHotEncoder`.\n",
    "At this stage:\n",
    "- each category becomes its own binary dimension,\n",
    "- no artificial ordering is introduced,\n",
    "- the output is a vector representation suitable for ML models.\n",
    "\n",
    "Finally, `VectorAssembler` is used to:\n",
    "- collect one or more encoded columns,\n",
    "- pack them into a single feature vector,\n",
    "- produce a clean, model-ready column.\n",
    "\n",
    "The *StringIndexer → OneHotEncoder → VectorAssembler* pattern is the standard approach for low-cardinality categorical features where interpretability and simplicity matter more than compactness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf03383-b43e-49fe-b2b5-631f992af2df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# StringIndexer\n",
    "string_cols = [\"gender\"]\n",
    "index_cols = [\"gender_index\"]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=string_cols, outputCols=index_cols, handleInvalid=\"skip\")\n",
    "string_indexer_model = string_indexer.fit(silver_df)\n",
    "silver_df = string_indexer_model.transform(silver_df)\n",
    "\n",
    "# Create a list of one-hot encoded feature names\n",
    "ohe_cols = [\"gender_encoded\"]\n",
    "\n",
    "# Instantiate the OneHotEncoder with the column lists\n",
    "ohe = OneHotEncoder(inputCols=index_cols, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "\n",
    "# Fit the OneHotEncoder on the indexed data\n",
    "ohe_model = ohe.fit(silver_df)\n",
    "\n",
    "# Transform indexed_df using the ohe_model\n",
    "silver_df = ohe_model.transform(silver_df)\n",
    "\n",
    "# Use VectorAssembler to assemble the selected one-hot encoded columns into a dense vector\n",
    "assembler = VectorAssembler(inputCols=ohe_cols, outputCol=\"gender_features\")\n",
    "silver_df = assembler.transform(silver_df)\n",
    "\n",
    "display(silver_df)\n",
    "\n",
    "silver_df = silver_df.drop(\"gender\", \"gender_index\", \"gender_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8612ca-8b49-4528-8ee4-9a752b9b218d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Categorical embeddings encoding\n",
    "\n",
    "We transform multiple related categorical columns into dense embeddings using pre-trained `Word2Vec` models stored in MLflow.\n",
    "\n",
    "First, for internet-related features:\n",
    "- We combine columns like `internet_service`, `online_security`, and others into a single sequence per customer.\n",
    "- The sequence is tokenized into an array so Word2Vec can process it.\n",
    "- The registered MLflow Word2Vec model is loaded and applied to generate `categorical_embedding` vectors.\n",
    "- Temporary columns used for tokenization are dropped after embedding.\n",
    "\n",
    "Next, for payment method:\n",
    "- We normalize the `payment_method` column by removing parentheses and split the string into tokens.\n",
    "- The corresponding MLflow Word2Vec model for payment methods is loaded.\n",
    "- Each row is transformed into a dense vector representing the payment feature.\n",
    "- Original and intermediate columns are dropped to keep the dataset clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0594fc30-7b0f-4a44-8444-00b8964ceb8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql.functions import split, concat_ws\n",
    "\n",
    "# Define categorical columns\n",
    "internet_categorical_columns = [\"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\"]\n",
    "silver_df = silver_df.withColumn(\"categorical_sequence\", concat_ws(\";\", *internet_categorical_columns))\n",
    "silver_df = silver_df.withColumn(\"categorical_tokens\", split(col(\"categorical_sequence\"), \";\"))\n",
    "\n",
    "model = mlflow.spark.load_model(\n",
    "    \"models:/ai_ml_in_practice.telco_customer_churn_silver.telco_word2vec_internet_services/1\",\n",
    "    dfs_tmpdir=\"/Volumes/ai_ml_in_practice/telco_customer_churn_silver/mlflow_tmp\"\n",
    ")\n",
    "\n",
    "# Generate embeddings for categorical columns\n",
    "silver_df = model.transform(silver_df)\n",
    "\n",
    "display(silver_df)\n",
    "\n",
    "silver_df = silver_df.drop(\"categorical_sequence\", \"categorical_tokens\", *internet_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ca284e-15a1-4d4c-83ab-d4b1bb40973d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql.functions import split, concat_ws, regexp_replace, col\n",
    "\n",
    "silver_df = (\n",
    "    silver_df\n",
    "    .withColumn(\"payment_normalized\", regexp_replace(col('payment_method'), r'[()]', ''))\n",
    "    .withColumn(\"payment_tokens\", split(col(\"payment_method\"), \" \"))\n",
    ")\n",
    "\n",
    "model = mlflow.spark.load_model(\n",
    "    \"models:/ai_ml_in_practice.telco_customer_churn_silver.telco_word2vec_payment_methods/1\",\n",
    "    dfs_tmpdir=\"/Volumes/ai_ml_in_practice/telco_customer_churn_silver/mlflow_tmp\"\n",
    ")\n",
    "\n",
    "# Generate embeddings for categorical columns\n",
    "silver_df = model.transform(silver_df)\n",
    "\n",
    "display(silver_df)\n",
    "\n",
    "silver_df = silver_df.drop(\"payment_method\", \"payment_normalized\", \"payment_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5798869a-0887-4b75-a512-6d82a7770872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature hashing\n",
    "\n",
    "In this step we handle tricky categorical combinations with **feature hashing**.\n",
    "\n",
    "- We first merge `phone_service` and `multiple_lines` into a single string per row.\n",
    "- Then we apply `FeatureHasher` to convert this combined category into a fixed-size numeric vector.\n",
    "- The `numFeatures=8` parameter keeps the embedding compact and avoids exploding dimensions.\n",
    "- This approach is useful when categories are numerous or inconsistent (like “No phone service” + “Yes”).\n",
    "- Finally, all intermediate and original columns are dropped, leaving only clean, hashed features.\n",
    "\n",
    "The result is a small, consistent numerical representation of complex categorical combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed51864-ebef-41e4-9c6f-846fb4ab7641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"phone_multiple_lines\",\n",
    "    concat_ws(\"_\", col(\"phone_service\"), col(\"multiple_lines\"))\n",
    ")\n",
    "\n",
    "hasher = FeatureHasher(\n",
    "    inputCols=[\"phone_multiple_lines\"],\n",
    "    outputCol=\"phone_multiple_lines_hashed\",\n",
    "    numFeatures=8\n",
    ")\n",
    "\n",
    "silver_df = hasher.transform(silver_df)\n",
    "display(silver_df)\n",
    "\n",
    "silver_df = silver_df.drop(\"phone_service\", \"multiple_lines\", \"phone_multiple_lines\", \"phone_multiple_lines_hashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f897f4f5-ad12-4ee5-8968-6120389d6e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ordinal encoding\n",
    "\n",
    "Here we encode an **ordinal categorical feature** (`contract`) into numbers.\n",
    "- We define an explicit order: `\"Month-to-month\" → 1`, `\"One year\" → 2`, `\"Two year\" → 3`.\n",
    "- A new column `contract_ordinal` is created with these numeric values.\n",
    "- The original string column is dropped to avoid duplication or accidental misuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bd217a-7295-49c3-a433-035a17625bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "ordinal_cat = \"contract\"\n",
    "\n",
    "ordered_list = [\n",
    "    \"Month-to-month\",\n",
    "    \"One year\",\n",
    "    \"Two year\"\n",
    "]\n",
    "\n",
    "ordinal_dict = {category: f\"{index+1}\" for index, category in enumerate(ordered_list)}\n",
    "\n",
    "silver_df = (\n",
    "    silver_df\n",
    "    .withColumn(f\"{ordinal_cat}_ordinal\", col(ordinal_cat))\n",
    "    .replace(to_replace=ordinal_dict, subset=[f\"{ordinal_cat}_ordinal\"]) \n",
    "    .withColumn(f\"{ordinal_cat}_ordinal\", col(f\"{ordinal_cat}_ordinal\").cast('int'))\n",
    ")\n",
    "\n",
    "display(silver_df)\n",
    "\n",
    "silver_df = silver_df.drop(ordinal_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78866ad-3654-4445-b6c8-556d1c684a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Numerical columns scaling\n",
    "\n",
    "This step scales continuous numeric features for ML models.\n",
    "- We first gather `tenure`, `monthly_charges`, and `total_charges` into a single vector using `VectorAssembler`.\n",
    "- `StandardScaler` then standardizes the vector: centering (mean=0) and scaling (unit variance).\n",
    "- The scaled output goes into `numeric_scaled`, making the data consistent and comparable across features.\n",
    "- Original columns are dropped to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8d7573-3162-4bd8-aac7-9b78634bf4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "numeric_cols = [\"tenure\", \"monthly_charges\", \"total_charges\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCol=\"numeric_features\"\n",
    ")\n",
    "\n",
    "silver_df = assembler.transform(silver_df)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features\",\n",
    "    outputCol=\"numeric_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(silver_df)\n",
    "silver_df = scaler_model.transform(silver_df)\n",
    "\n",
    "display(silver_df.select(*numeric_cols, \"numeric_scaled\"))\n",
    "\n",
    "silver_df = silver_df.drop(\"numeric_features\", *numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be93badd-085f-4389-b829-5a1ff05f1466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Final assembly\n",
    "\n",
    "In this final step we combine all prepared features into a single vector for modeling.\n",
    "- We collect all columns except the target (`churn`) as inputs.\n",
    "- `VectorAssembler` merges them into one dense feature column, `customer_features`.\n",
    "- Intermediate columns are dropped to keep the DataFrame clean and ML-ready.\n",
    "\n",
    "Why:\n",
    "- Spark ML models expect a single vector column for features.\n",
    "- Assembling everything at the end simplifies the pipeline and avoids accidental column misuse.\n",
    "- Ensures consistent input structure for training and inference.\n",
    "\n",
    "Result: a DataFrame with `churn` as the label and `customer_features` as a unified input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16e3291-1fc4-4dce-b9da-d059910957ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembly_columns = [column for column in silver_df.columns if column != \"churn\"]\n",
    "\n",
    "print(assembly_columns)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=assembly_columns,\n",
    "    outputCol=\"customer_features\"\n",
    ")\n",
    "\n",
    "silver_df = assembler.transform(silver_df)\n",
    "silver_df = silver_df.drop(*assembly_columns)\n",
    "\n",
    "display(silver_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.1_telco_feature_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
