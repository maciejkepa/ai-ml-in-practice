{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5428289c-6c16-4b42-b3e0-60514f6b40f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf1999f-0f78-49b8-9681-b28f5dd2b550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../environment/prepare_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cab3bb-e141-4c91-8457-e89b8ff5fac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Data drift detection\n",
    "\n",
    "This notebook detects feature-level data drift between training and inference datasets using PySpark and statistical population metrics.\n",
    "\n",
    "The purpose of this notebook is to ensure that the data used during model inference remains statistically consistent with the data used during training, before any model quality degradation becomes visible.\n",
    "\n",
    "In detail, this notebook:\n",
    "- Accepts two input parameters: the training table and the inference table (Delta tables).\n",
    "- Loads both datasets using Spark and identifies numerical feature columns automatically.\n",
    "- Uses the training dataset as a reference distribution to avoid data leakage.\n",
    "- Computes Population Stability Index (PSI) for each numerical feature using distributed PySpark operations.\n",
    "- Classifies drift severity per feature (no drift, moderate drift, severe drift) based on industry-standard thresholds.\n",
    "- Logs drift metrics and metadata to MLflow for auditability, historical tracking, and correlation with deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a087e7-9ce2-40d6-9b9a-88d445eb34f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the data. Use dbutils.widgets for latter parametrization in Databricks Jobs\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "dbutils.widgets.text(\"train_table\", \"ai_ml_in_practice.telco_customer_churn_silver.telco_silver\")\n",
    "dbutils.widgets.text(\"inference_table\", \"ai_ml_in_practice.telco_customer_churn_silver.telco_silver\")\n",
    "dbutils.widgets.text(\"num_bins\", \"10\")\n",
    "\n",
    "TRAIN_TABLE = dbutils.widgets.get(\"train_table\")\n",
    "INFERENCE_TABLE = dbutils.widgets.get(\"inference_table\")\n",
    "NUM_BINS = int(dbutils.widgets.get(\"num_bins\"))\n",
    "\n",
    "train_df = spark.read.table(TRAIN_TABLE)\n",
    "inf_df = spark.read.table(INFERENCE_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0ae996-9d4b-44ac-a09e-9803654ec554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Prepare function to calculate Population Stability Index (PSI)\n",
    "def compute_psi(train_df, inf_df, col_name, num_bins=10, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Population Stability Index (PSI) for a single numerical feature\n",
    "    between a training dataset and an inference dataset using PySpark.\n",
    "\n",
    "    The function uses the training dataset as a reference distribution to define\n",
    "    bucket boundaries (quantile-based binning) in order to avoid data leakage.\n",
    "    It then compares the relative frequency of observations in each bucket\n",
    "    between training and inference data to quantify distributional shift.\n",
    "\n",
    "    PSI is commonly used to detect feature-level data drift in production\n",
    "    machine learning systems.\n",
    "\n",
    "    Parameters:\n",
    "        train_df (pyspark.sql.DataFrame): Spark DataFrame containing the training data.\n",
    "        inf_df (pyspark.sql.DataFrame): Spark DataFrame containing the inference data.\n",
    "        col_name (str): Name of the numerical feature column to evaluate.\n",
    "        num_bins (int): Number of quantile-based buckets to use for PSI calculation.\n",
    "        eps (float): Small constant added to bucket proportions to avoid division\n",
    "                     by zero and logarithm of zero.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed PSI value for the given feature. Lower values indicate\n",
    "               similar distributions, while higher values indicate stronger drift.\n",
    "    \"\"\"\n",
    "\n",
    "    quantiles = train_df.approxQuantile(col_name, \n",
    "                                         [i / num_bins for i in range(1, num_bins)],\n",
    "                                         0.01)\n",
    "\n",
    "    splits = [-float(\"inf\")] + quantiles + [float(\"inf\")]\n",
    "\n",
    "    bucketizer = F.when(F.col(col_name) < splits[1], 0)\n",
    "    for i in range(1, len(splits) - 1):\n",
    "        bucketizer = bucketizer.when(\n",
    "            (F.col(col_name) >= splits[i]) & (F.col(col_name) < splits[i+1]),\n",
    "            i\n",
    "        )\n",
    "    bucketizer = bucketizer.otherwise(len(splits) - 2)\n",
    "\n",
    "    train_hist = (\n",
    "        train_df\n",
    "        .withColumn(\"bucket\", bucketizer)\n",
    "        .groupBy(\"bucket\")\n",
    "        .count()\n",
    "        .withColumn(\"train_pct\", F.col(\"count\") / train_df.count())\n",
    "        .select(\"bucket\", \"train_pct\")\n",
    "    )\n",
    "\n",
    "    inf_hist = (\n",
    "        inf_df\n",
    "        .withColumn(\"bucket\", bucketizer)\n",
    "        .groupBy(\"bucket\")\n",
    "        .count()\n",
    "        .withColumn(\"inf_pct\", F.col(\"count\") / inf_df.count())\n",
    "        .select(\"bucket\", \"inf_pct\")\n",
    "    )\n",
    "\n",
    "    joined = (\n",
    "        train_hist\n",
    "        .join(inf_hist, on=\"bucket\", how=\"outer\")\n",
    "        .fillna(eps)\n",
    "    )\n",
    "\n",
    "    psi_df = joined.withColumn(\n",
    "        \"psi\",\n",
    "        (F.col(\"train_pct\") - F.col(\"inf_pct\")) *\n",
    "        F.log(F.col(\"train_pct\") / F.col(\"inf_pct\"))\n",
    "    )\n",
    "\n",
    "    psi_value = psi_df.agg(F.sum(\"psi\")).first()[0]\n",
    "\n",
    "    return float(psi_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d2b6ee-7a22-474a-987c-ba810324359f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_cols = [\n",
    "    field.name\n",
    "    for field in train_df.schema.fields\n",
    "    if isinstance(field.dataType, NumericType)\n",
    "]\n",
    "\n",
    "# Start mlflow run context to run the test and log metrics\n",
    "with mlflow.start_run(run_name=\"pyspark_feature_drift\"):\n",
    "    mlflow.log_param(\"train_table\", TRAIN_TABLE)\n",
    "    mlflow.log_param(\"inference_table\", INFERENCE_TABLE)\n",
    "    mlflow.log_param(\"num_bins\", NUM_BINS)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for col_name in numeric_cols:\n",
    "        psi = compute_psi(train_df, inf_df, col_name, NUM_BINS)\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": col_name,\n",
    "            \"psi\": psi,\n",
    "            \"drift_level\": (\n",
    "                \"NO_DRIFT\" if psi < 0.1 else\n",
    "                \"MODERATE\" if psi < 0.2 else\n",
    "                \"SEVERE\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "    psi_df = spark.createDataFrame(results)\n",
    "    display(psi_df.orderBy(F.desc(\"psi\")))\n",
    "\n",
    "    metrics = {\n",
    "        f\"psi_{row['feature']}\": row[\"psi\"]\n",
    "        for row in psi_df.collect()\n",
    "    }\n",
    "\n",
    "    mlflow.log_metrics(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.3_telco_data_drift",
   "widgets": {
    "inference_table": {
     "currentValue": "ai_ml_in_practice.telco_customer_churn_silver.telco_silver",
     "nuid": "664317ae-1005-4b09-bb2b-1d23931691dd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ai_ml_in_practice.telco_customer_churn_silver.telco_silver",
      "label": null,
      "name": "inference_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ai_ml_in_practice.telco_customer_churn_silver.telco_silver",
      "label": null,
      "name": "inference_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "num_bins": {
     "currentValue": "10",
     "nuid": "bedcca54-8ef5-402c-9a22-69f237aae86f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": null,
      "name": "num_bins",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": null,
      "name": "num_bins",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "train_table": {
     "currentValue": "ai_ml_in_practice.telco_customer_churn_silver.telco_silver",
     "nuid": "63589c63-e9fb-4a0d-99e8-5f751a91d76a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ai_ml_in_practice.telco_customer_churn_silver.telco_silver",
      "label": null,
      "name": "train_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ai_ml_in_practice.telco_customer_churn_silver.telco_silver",
      "label": null,
      "name": "train_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
